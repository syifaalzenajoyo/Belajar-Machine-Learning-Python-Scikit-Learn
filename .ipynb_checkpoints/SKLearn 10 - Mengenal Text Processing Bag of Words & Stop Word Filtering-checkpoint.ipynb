{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SKLearn 10 - Mengenal Text Processing: Bag of Words & Stop Word Filtering\n",
    "\n",
    "Machine tidak dapat memahami teks dengan baik. Namun, dalam machine learning terdapat NLP (Natural Language Processing) yang merupakan bidang yang secara spesifik membahas mengenai dataset text. Terdapat teknik yang sering digunakan untuk melakukan deature expression dari dataset text, 2 diantaranya adalah:\n",
    "- Bag of Words\n",
    "- Stop Word Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Bag of Words Model* sebagai Representasi Text\n",
    "Bag of Words merupakan teknik text processing yang memiliki beberapa karakteristik seperti di bawah ini:\n",
    "- Menyederhanakan representasi text sebagai sekumpulan kata\n",
    "- Mengabaikan grammar dan posisi tiap kata pada kalimat\n",
    "- Text akan dikonversi menjadi lowercase dan tanda baca akan diabaikan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "Untuk menerapkan teknik bag of words, langkah pertama yang perlu kita lakukan adalah menyiapkan dataset text yang dikenal dengan istilah corpus. Dataset text tersebut berisikan sekumpulan kalimat pendek yang ditampung oleh suatu list. Pada kesempatan kali ini, dataset text tersebut akan kita tampung dalam variabel `corpus`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Linux has been around since the mid-1990s.',\n",
       " 'Linux distributions include the Linux kernel.',\n",
       " 'Linux is one of the most prominent open-source software.']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\n",
    "    'Linux has been around since the mid-1990s.',\n",
    "    'Linux distributions include the Linux kernel.',\n",
    "    'Linux is one of the most prominent open-source software.'\n",
    "]\n",
    "\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Bag Of Words Model* dengan `CountVectorizer`\n",
    "\n",
    "Pada SKLearn, untuk melakukan feature expression melalui Bag of Words model dapat diterapkan dengan memanfatkan CountVectorizer.\n",
    "- Import module `CountVectorizer` dari `sklearn.feature_extraction.text`\n",
    "- Membentuk object `CountVectorizer()` yang ditampung oleh variabel `vectorizer`\n",
    "- Memanggil method fit_transform dari object `vectorizer` yang akan kita terapkan terhadap `corpus`\n",
    "- Menerapkan method `todense()`untuk mengonversi hasil dari object `CountVecorizer()` menjadi suatu array 2 dimensi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1],\n",
       "        [0, 0, 0, 1, 0, 1, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1]],\n",
       "       dtype=int64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorized_X = vectorizer.fit_transform(corpus).todense()\n",
    "vectorized_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bagian di atas merupakan hasil array 2 dimensi dimana setiap barisnya merepresentasikan kemunculan setiap kata yang ada pada kalimat dalam corpus. Urutan kata yang direpresentasikan sudah tersusun secara alfabetikal. Untuk melihat isi dari vectorizer kita dapat memanggil `vectorizer.get_feature_names()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1990s',\n",
       " 'around',\n",
       " 'been',\n",
       " 'distributions',\n",
       " 'has',\n",
       " 'include',\n",
       " 'is',\n",
       " 'kernel',\n",
       " 'linux',\n",
       " 'mid',\n",
       " 'most',\n",
       " 'of',\n",
       " 'one',\n",
       " 'open',\n",
       " 'prominent',\n",
       " 'since',\n",
       " 'software',\n",
       " 'source',\n",
       " 'the']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Setiap kata yang ditampung di dalam bag juga dikenal dengan istilah token\n",
    "- Setiap token tidak diurutkan berdasarkan urutannya di dalam kalimat, namun diurutkan secara alfabetikal.\n",
    "- Semua case pada setiap token menjadi lower case\n",
    "- Array yang berisikan sekumpulan nilai bilangan 0, 1, 2, dll merepresentasikan jumlah kemunculan setiap token pada kalimat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Euclidean Distance* untuk Mengukur Kedekatan/Jarak Antar Dokumen (Vector)\n",
    "\n",
    "Bag of Words dapat membantu proses training dari suatu model atau algoritma machine learning. Dengan bag of words, suatu algoritma machine learning dapat menjadi lebih mudah dalam mengukur euclidean distance atau jarak kedekatan/kemiripan antar dokumen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jarak dokumen 1 dan 2: [[3.16227766]]\n",
      "Jarak dokumen 1 dan 3: [[3.74165739]]\n",
      "Jarak dokumen 2 dan 3: [[3.46410162]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "for i in range(len(vectorized_X)):\n",
    "    for j in range(i, len(vectorized_X)):\n",
    "        if i == j:\n",
    "            continue\n",
    "        jarak = euclidean_distances(vectorized_X[i], vectorized_X[j])\n",
    "        print(f'Jarak dokumen {i+1} dan {j+1}: {jarak}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Stop  Word Filtering* pada *Text*\n",
    "*Stop Word Filtering* merupakan teknik text processing yang menyederhanakan representasi text dengan mengabaikan beberapa kata seperti determiners (the, a, an), auxiliary verbs (do, be, will), dan prepositions (on, in, at).\n",
    "\n",
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Linux has been around since the mid-1990s.',\n",
       " 'Linux distributions include the Linux kernel.',\n",
       " 'Linux is one of the most prominent open-source software.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Stop Word Filtering* dengan `CountVectorizer`\n",
    "Stop Word Filtering juga dapat diterapkan dengan memanfaatkan `CountVectorizer`.\n",
    "- Import module `CountVectorizer` dari `sklearn.feature_extraction.text`\n",
    "- Membentuk object `CountVectorizer()` yang disertai oleh parameter `stop_words='english'`. Parameter tersebut disertakan untuk melakukan stop word filtering terhadap kalimat dalam bahasa Inggris. Hasil object `CountVectorizer` akan ditampung oleh variabel `vectorizer`\n",
    "- Memanggil method fit_transform dari object `vectorizer` yang akan kita terapkan terhadap `corpus`\n",
    "- Menerapkan method `todense()`untuk mengonversi hasil dari object `CountVecorizer()` menjadi suatu array 2 dimensi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
       "        [0, 1, 1, 1, 2, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 0, 1, 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "vectorized_X = vectorizer.fit_transform(corpus).todense()\n",
    "vectorized_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bagian di atas merupakan hasil array 2 dimensi dimana setiap barisnya merepresentasikan kemunculan setiap kata yang ada pada kalimat dalam corpus. Untuk melihat isi dari vectorizer kita dapat memanggil `vectorizer.get_feature_names()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1990s',\n",
       " 'distributions',\n",
       " 'include',\n",
       " 'kernel',\n",
       " 'linux',\n",
       " 'mid',\n",
       " 'open',\n",
       " 'prominent',\n",
       " 'software',\n",
       " 'source']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Setiap token tidak diurutkan berdasarkan urutannya di dalam kalimat, namun diurutkan secara alfabetikal.\n",
    "- Ukuran array yang dihasilkan menjadi lebih kecil karena telah dilakukan stop word filtering sehingga kata-kata seperti \n",
    "`around, been, has, is, most, of, one, since, dan the` dikeluarkan karena merupakan stop word.\n",
    "- Semua case pada setiap token menjadi lower case\n",
    "- Array yang berisikan sekumpulan nilai bilangan 0, 1, 2, dll merepresentasikan jumlah kemunculan setiap token pada kalimat."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
